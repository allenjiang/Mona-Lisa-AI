{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal is to answer the question: can a machine detect a person's gender based on their tweet?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Get twitter data\n",
    "2. Clean\n",
    "3. Analyze and visualize\n",
    "4. Build model\n",
    "5. Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tweepy as tw\n",
    "import seaborn as sns\n",
    "import json\n",
    "import pprint\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk import PorterStemmer\n",
    "import textblob\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report,accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweepy.api.API"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connect to twitter API\n",
    "path_auth = '/Users/allenj/Documents/Keys/auth_twitter.json'\n",
    "auth = json.loads(open(path_auth).read())\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "my_consumer_key = auth['my_consumer_key']\n",
    "my_consumer_secret = auth['my_consumer_secret']\n",
    "my_access_token = auth['your_access_token']\n",
    "my_access_token_secret = auth['my_access_token_secret']\n",
    "\n",
    "auth = tw.OAuthHandler(my_consumer_key, my_consumer_secret)\n",
    "auth.set_access_token(my_access_token, my_access_token_secret)\n",
    "api = tw.API(auth)\n",
    "\n",
    "type(api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Get Twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "      <th>followers_millions</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>jk_rowling</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>1</td>\n",
       "      <td>15.00</td>\n",
       "      <td>Author</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>LiamPayne</td>\n",
       "      <td>Liam Payne</td>\n",
       "      <td>0</td>\n",
       "      <td>33.00</td>\n",
       "      <td>Musician</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>chefannc</td>\n",
       "      <td>Anne Cooper</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>Chef</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>wizkhalifa</td>\n",
       "      <td>Wiz Khalifa</td>\n",
       "      <td>0</td>\n",
       "      <td>36.00</td>\n",
       "      <td>Musician</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>Harry_Styles</td>\n",
       "      <td>Harry Styles</td>\n",
       "      <td>0</td>\n",
       "      <td>34.00</td>\n",
       "      <td>Musician</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>ladygaga</td>\n",
       "      <td>Lady Gaga</td>\n",
       "      <td>1</td>\n",
       "      <td>81.00</td>\n",
       "      <td>Musician</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>carmeloanthony</td>\n",
       "      <td>Carmelo Anthony</td>\n",
       "      <td>0</td>\n",
       "      <td>9.00</td>\n",
       "      <td>Athlete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>iSmashFizzle</td>\n",
       "      <td>Ashley C. Ford</td>\n",
       "      <td>1</td>\n",
       "      <td>0.19</td>\n",
       "      <td>Author</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>rihanna</td>\n",
       "      <td>Rihanna</td>\n",
       "      <td>1</td>\n",
       "      <td>96.00</td>\n",
       "      <td>Musician</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>0</td>\n",
       "      <td>82.00</td>\n",
       "      <td>Politician</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>justinbieber</td>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>0</td>\n",
       "      <td>111.00</td>\n",
       "      <td>Musician</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>1</td>\n",
       "      <td>28.00</td>\n",
       "      <td>Politician</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>kenyanpundit</td>\n",
       "      <td>Ory Okolloh</td>\n",
       "      <td>1</td>\n",
       "      <td>0.40</td>\n",
       "      <td>Executive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>aliciakeys</td>\n",
       "      <td>Alicia Keys</td>\n",
       "      <td>1</td>\n",
       "      <td>30.00</td>\n",
       "      <td>Musician</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>justdemi</td>\n",
       "      <td>Demi Moore</td>\n",
       "      <td>1</td>\n",
       "      <td>5.00</td>\n",
       "      <td>Actor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>UnhealthyTruth</td>\n",
       "      <td>Robyn O'Brien</td>\n",
       "      <td>1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>Entrepreneur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>MileyCyrus</td>\n",
       "      <td>Miley Cyrus</td>\n",
       "      <td>1</td>\n",
       "      <td>44.00</td>\n",
       "      <td>Musician</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>arrahman</td>\n",
       "      <td>A.R.Rahman</td>\n",
       "      <td>0</td>\n",
       "      <td>23.00</td>\n",
       "      <td>Musician</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>KellyMcGonigal</td>\n",
       "      <td>Kelly McGonigal</td>\n",
       "      <td>1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>Psychologist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>SrBachchan</td>\n",
       "      <td>Amitabh Bachchan</td>\n",
       "      <td>0</td>\n",
       "      <td>42.00</td>\n",
       "      <td>Actor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               user              name  gender  followers_millions  \\\n",
       "53       jk_rowling      J.K. Rowling       1               15.00   \n",
       "38        LiamPayne        Liam Payne       0               33.00   \n",
       "79         chefannc       Anne Cooper       1                0.01   \n",
       "32       wizkhalifa       Wiz Khalifa       0               36.00   \n",
       "35     Harry_Styles      Harry Styles       0               34.00   \n",
       "7          ladygaga         Lady Gaga       1               81.00   \n",
       "54   carmeloanthony   Carmelo Anthony       0                9.00   \n",
       "65     iSmashFizzle    Ashley C. Ford       1                0.19   \n",
       "3           rihanna           Rihanna       1               96.00   \n",
       "6   realDonaldTrump      Donald Trump       0               82.00   \n",
       "1      justinbieber     Justin Bieber       0              111.00   \n",
       "42   HillaryClinton   Hillary Clinton       1               28.00   \n",
       "61     kenyanpundit       Ory Okolloh       1                0.40   \n",
       "39       aliciakeys       Alicia Keys       1               30.00   \n",
       "56         justdemi        Demi Moore       1                5.00   \n",
       "69   UnhealthyTruth     Robyn O'Brien       1                0.04   \n",
       "22       MileyCyrus       Miley Cyrus       1               44.00   \n",
       "46         arrahman        A.R.Rahman       0               23.00   \n",
       "71   KellyMcGonigal   Kelly McGonigal       1                0.04   \n",
       "25       SrBachchan  Amitabh Bachchan       0               42.00   \n",
       "\n",
       "        activity  \n",
       "53        Author  \n",
       "38      Musician  \n",
       "79          Chef  \n",
       "32      Musician  \n",
       "35      Musician  \n",
       "7       Musician  \n",
       "54       Athlete  \n",
       "65        Author  \n",
       "3       Musician  \n",
       "6     Politician  \n",
       "1       Musician  \n",
       "42    Politician  \n",
       "61     Executive  \n",
       "39      Musician  \n",
       "56         Actor  \n",
       "69  Entrepreneur  \n",
       "22      Musician  \n",
       "46      Musician  \n",
       "71  Psychologist  \n",
       "25         Actor  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload list of desired users\n",
    "# Gender 0 = male, 1 = female\n",
    "users = pd.read_csv('../Data/twitter-users.csv')\n",
    "users.sample(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    47\n",
       "0    36\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get collection of tweets from these usernames and store it into a new dataframe\n",
    "list = []\n",
    "\n",
    "for index, row in users.iterrows():\n",
    "    tweets = api.user_timeline(screen_name=row['user'], count=200, include_rts=False)\n",
    "    users_text = [[tweet.user.screen_name, tweet.text, row['gender']] for tweet in tweets]\n",
    "    tweet_text = pd.DataFrame(data=users_text, \n",
    "                        columns=[\"user\", \"text\", \"gender\"])\n",
    "    list.append(tweet_text)\n",
    "\n",
    "# Merge the list    \n",
    "tweets = pd.concat(list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>gender</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>aaker</td>\n",
       "      <td>@drewlewisjr @dickc @LeslieBlodgett @davidhorn...</td>\n",
       "      <td>1</td>\n",
       "      <td>Um?!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>developingjen</td>\n",
       "      <td>Really proud of @tristanharris and this new cl...</td>\n",
       "      <td>1</td>\n",
       "      <td>Really proud of  and this new clarity and dire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>jtimberlake</td>\n",
       "      <td>.@PilgrimageFest is back for year 5 this weeke...</td>\n",
       "      <td>0</td>\n",
       "      <td>. is back for year 5 this weekend. Get out the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>kenyanpundit</td>\n",
       "      <td>@tutasema Yes but if there‚Äôs any indication it...</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes but if there‚Äôs any indication it could go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>AdamMGrant</td>\n",
       "      <td>Don't give leaders a pass for the choices they...</td>\n",
       "      <td>0</td>\n",
       "      <td>Don't give leaders a pass for the choices they...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>jennpozner</td>\n",
       "      <td>@DrMkWalters @jamiaw @lizzwinstead @ZerlinaMax...</td>\n",
       "      <td>1</td>\n",
       "      <td>Thank you!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>Adele</td>\n",
       "      <td>I really hope Laura Marling wins the mercury t...</td>\n",
       "      <td>1</td>\n",
       "      <td>I really hope Laura Marling wins the mercury t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>Adele</td>\n",
       "      <td>Last night was mad! I had such a great time......</td>\n",
       "      <td>1</td>\n",
       "      <td>Last night was mad! I had such a great time......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>ruthreichl</td>\n",
       "      <td>You'll have to wait for today's Gift Guide sug...</td>\n",
       "      <td>1</td>\n",
       "      <td>You'll have to wait for today's Gift Guide sug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>carmeloanthony</td>\n",
       "      <td>.@roadto2022 #Qatar #GameOnDoha #STAYME7O http...</td>\n",
       "      <td>0</td>\n",
       "      <td>.    https://t.co/6RJzM3wPgp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>Lissarankin</td>\n",
       "      <td>https://t.co/IbTCa5qXaj</td>\n",
       "      <td>1</td>\n",
       "      <td>https://t.co/IbTCa5qXaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>shakira</td>\n",
       "      <td>Soccer mom and skate mom! https://t.co/nlUFLVyykd</td>\n",
       "      <td>1</td>\n",
       "      <td>Soccer mom and skate mom! https://t.co/nlUFLVyykd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>susanmcp1</td>\n",
       "      <td>Yes, in 1941, the @NYTimes ran an excerpts of ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes, in 1941, the  ran an excerpts of 'Mein Ka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>LilTunechi</td>\n",
       "      <td>Katch me in Cincinnati Monday !! Luv! ü§ôüèæ</td>\n",
       "      <td>0</td>\n",
       "      <td>Katch me in Cincinnati Monday !! Luv! ü§ôüèæ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>neymarjr</td>\n",
       "      <td>Que Deus nos aben√ßoe e nos proteja üôèüèΩ‚öΩÔ∏è https:...</td>\n",
       "      <td>0</td>\n",
       "      <td>Que Deus nos aben√ßoe e nos proteja üôèüèΩ‚öΩÔ∏è https:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>Louis_Tomlinson</td>\n",
       "      <td>@glxtchcity @earthIouis @jadoreloueh Actually ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Actually not a bad idea. I'll look through ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>justinbieber</td>\n",
       "      <td>https://t.co/UAUqHJhb4M #stuckwithu</td>\n",
       "      <td>0</td>\n",
       "      <td>https://t.co/UAUqHJhb4M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>rihanna</td>\n",
       "      <td>Nobody:\\nMe: Album coming in 2019\\nNavy in Jul...</td>\n",
       "      <td>1</td>\n",
       "      <td>Nobody:\\nMe: Album coming in 2019\\nNavy in Jul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>Cristiano</td>\n",
       "      <td>There is only one shampoo for me.\\n #ClearMen ...</td>\n",
       "      <td>0</td>\n",
       "      <td>There is only one shampoo for me.\\n  \\n   http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>Adele</td>\n",
       "      <td>@onlyhowell I'm SO excited you're coming x</td>\n",
       "      <td>1</td>\n",
       "      <td>I'm SO excited you're coming x</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                user                                               text  \\\n",
       "30             aaker  @drewlewisjr @dickc @LeslieBlodgett @davidhorn...   \n",
       "87     developingjen  Really proud of @tristanharris and this new cl...   \n",
       "65       jtimberlake  .@PilgrimageFest is back for year 5 this weeke...   \n",
       "27      kenyanpundit  @tutasema Yes but if there‚Äôs any indication it...   \n",
       "198       AdamMGrant  Don't give leaders a pass for the choices they...   \n",
       "150       jennpozner  @DrMkWalters @jamiaw @lizzwinstead @ZerlinaMax...   \n",
       "116            Adele  I really hope Laura Marling wins the mercury t...   \n",
       "62             Adele  Last night was mad! I had such a great time......   \n",
       "125       ruthreichl  You'll have to wait for today's Gift Guide sug...   \n",
       "90    carmeloanthony  .@roadto2022 #Qatar #GameOnDoha #STAYME7O http...   \n",
       "188      Lissarankin                            https://t.co/IbTCa5qXaj   \n",
       "65           shakira  Soccer mom and skate mom! https://t.co/nlUFLVyykd   \n",
       "45         susanmcp1  Yes, in 1941, the @NYTimes ran an excerpts of ...   \n",
       "143       LilTunechi           Katch me in Cincinnati Monday !! Luv! ü§ôüèæ   \n",
       "87          neymarjr  Que Deus nos aben√ßoe e nos proteja üôèüèΩ‚öΩÔ∏è https:...   \n",
       "155  Louis_Tomlinson  @glxtchcity @earthIouis @jadoreloueh Actually ...   \n",
       "68      justinbieber                https://t.co/UAUqHJhb4M #stuckwithu   \n",
       "158          rihanna  Nobody:\\nMe: Album coming in 2019\\nNavy in Jul...   \n",
       "114        Cristiano  There is only one shampoo for me.\\n #ClearMen ...   \n",
       "56             Adele         @onlyhowell I'm SO excited you're coming x   \n",
       "\n",
       "     gender                                         clean_text  \n",
       "30        1                                               Um?!  \n",
       "87        1  Really proud of  and this new clarity and dire...  \n",
       "65        0  . is back for year 5 this weekend. Get out the...  \n",
       "27        1   Yes but if there‚Äôs any indication it could go...  \n",
       "198       0  Don't give leaders a pass for the choices they...  \n",
       "150       1                                         Thank you!  \n",
       "116       1  I really hope Laura Marling wins the mercury t...  \n",
       "62        1  Last night was mad! I had such a great time......  \n",
       "125       1  You'll have to wait for today's Gift Guide sug...  \n",
       "90        0                       .    https://t.co/6RJzM3wPgp  \n",
       "188       1                            https://t.co/IbTCa5qXaj  \n",
       "65        1  Soccer mom and skate mom! https://t.co/nlUFLVyykd  \n",
       "45        1  Yes, in 1941, the  ran an excerpts of 'Mein Ka...  \n",
       "143       0           Katch me in Cincinnati Monday !! Luv! ü§ôüèæ  \n",
       "87        0  Que Deus nos aben√ßoe e nos proteja üôèüèΩ‚öΩÔ∏è https:...  \n",
       "155       0     Actually not a bad idea. I'll look through ...  \n",
       "68        0                           https://t.co/UAUqHJhb4M   \n",
       "158       1  Nobody:\\nMe: Album coming in 2019\\nNavy in Jul...  \n",
       "114       0  There is only one shampoo for me.\\n  \\n   http...  \n",
       "56        1                     I'm SO excited you're coming x  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user            gender\n",
       "AdamMGrant      0         200\n",
       "Adele           1         194\n",
       "AnushkaSharma   1         156\n",
       "ArianaGrande    1         107\n",
       "AvrilLavigne    1         135\n",
       "                         ... \n",
       "staceyannchin   1         168\n",
       "susanmcp1       1          60\n",
       "taylorswift13   1         189\n",
       "unhealthytruth  1         151\n",
       "wizkhalifa      0          97\n",
       "Length: 83, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of datapoints per person\n",
    "tweets.groupby([\"user\", \"gender\"]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.463736\n",
       "1    0.536264\n",
       "Name: gender, dtype: float64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check percentages for each gender\n",
    "# 0 = male, 1 = female\n",
    "tweets.gender.value_counts(normalize=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>BarackObama</td>\n",
       "      <td>We‚Äôve seen the power that our voices have when...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>BarackObama</td>\n",
       "      <td>On National Gun Violence Awareness Day, we #We...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>BarackObama</td>\n",
       "      <td>Third, every city in this country should be a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>BarackObama</td>\n",
       "      <td>Second, every mayor should review their use of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>BarackObama</td>\n",
       "      <td>First, there are specific evidence-based refor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>developingjen</td>\n",
       "      <td>Fascinating. // This is Your Brain on LSD http...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>developingjen</td>\n",
       "      <td>THIS! // Stacy Brown-Philpot tapped as new Tas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>developingjen</td>\n",
       "      <td>Sean Parker, a Facebook and Napster Pioneer, t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>developingjen</td>\n",
       "      <td>@harper Christian Loffler, Veiled Grey. Massiv...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>developingjen</td>\n",
       "      <td>@harper Peter Broderick: With Notes In My Ears...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12919 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              user                                               text  gender\n",
       "0      BarackObama  We‚Äôve seen the power that our voices have when...       0\n",
       "1      BarackObama  On National Gun Violence Awareness Day, we #We...       0\n",
       "2      BarackObama  Third, every city in this country should be a ...       0\n",
       "3      BarackObama  Second, every mayor should review their use of...       0\n",
       "4      BarackObama  First, there are specific evidence-based refor...       0\n",
       "..             ...                                                ...     ...\n",
       "190  developingjen  Fascinating. // This is Your Brain on LSD http...       1\n",
       "191  developingjen  THIS! // Stacy Brown-Philpot tapped as new Tas...       1\n",
       "192  developingjen  Sean Parker, a Facebook and Napster Pioneer, t...       1\n",
       "193  developingjen  @harper Christian Loffler, Veiled Grey. Massiv...       1\n",
       "194  developingjen  @harper Peter Broderick: With Notes In My Ears...       1\n",
       "\n",
       "[12919 rows x 3 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>gender</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>developingjen</td>\n",
       "      <td>Best one yet // Why the Conversation About Fur...</td>\n",
       "      <td>1</td>\n",
       "      <td>Best one yet // Why the Conversation About Fur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>developingjen</td>\n",
       "      <td>Recent reflections that are really impacting m...</td>\n",
       "      <td>1</td>\n",
       "      <td>Recent reflections that are really impacting m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>developingjen</td>\n",
       "      <td>I love this! https://t.co/rRmaDxjqRg</td>\n",
       "      <td>1</td>\n",
       "      <td>I love this! https://t.co/rRmaDxjqRg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>developingjen</td>\n",
       "      <td>Really great piece by @dgh.  So much wisdom. h...</td>\n",
       "      <td>1</td>\n",
       "      <td>Really great piece by .  So much wisdom. https...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>developingjen</td>\n",
       "      <td>The Further Future festival last weekend got m...</td>\n",
       "      <td>1</td>\n",
       "      <td>The Further Future festival last weekend got m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>developingjen</td>\n",
       "      <td>@iRowan thank you again for this :).</td>\n",
       "      <td>1</td>\n",
       "      <td>thank you again for this :).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>developingjen</td>\n",
       "      <td>cc @rosenstein  https://t.co/JJk8xKeihx</td>\n",
       "      <td>1</td>\n",
       "      <td>cc   https://t.co/JJk8xKeihx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>developingjen</td>\n",
       "      <td>Thank you @iRowan for making this your cover s...</td>\n",
       "      <td>1</td>\n",
       "      <td>Thank you  for making this your cover story fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>developingjen</td>\n",
       "      <td>Fascinating // This Mark Cuban-Backed Startup ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Fascinating // This Mark Cuban-Backed Startup ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>developingjen</td>\n",
       "      <td>BBC News - Italian court rules food theft 'not...</td>\n",
       "      <td>1</td>\n",
       "      <td>BBC News - Italian court rules food theft 'not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>developingjen</td>\n",
       "      <td>Best track ever? https://t.co/cqSHuILNVJ cc @h...</td>\n",
       "      <td>1</td>\n",
       "      <td>Best track ever? https://t.co/cqSHuILNVJ cc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>developingjen</td>\n",
       "      <td>What If We Just Gave Poor People a Basic Incom...</td>\n",
       "      <td>1</td>\n",
       "      <td>What If We Just Gave Poor People a Basic Incom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>developingjen</td>\n",
       "      <td>This is exciting https://t.co/hMCUA74bdI</td>\n",
       "      <td>1</td>\n",
       "      <td>This is exciting https://t.co/hMCUA74bdI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>developingjen</td>\n",
       "      <td>#NowPlaying Relax &amp;amp; Focus by Spotify // Pe...</td>\n",
       "      <td>1</td>\n",
       "      <td>Relax  Focus by Spotify // Perfect playlist w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>developingjen</td>\n",
       "      <td>#vacation https://t.co/xQCKam3LBX</td>\n",
       "      <td>1</td>\n",
       "      <td>https://t.co/xQCKam3LBX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>developingjen</td>\n",
       "      <td>Fascinating. // This is Your Brain on LSD http...</td>\n",
       "      <td>1</td>\n",
       "      <td>Fascinating. // This is Your Brain on LSD http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>developingjen</td>\n",
       "      <td>THIS! // Stacy Brown-Philpot tapped as new Tas...</td>\n",
       "      <td>1</td>\n",
       "      <td>THIS! // Stacy Brown-Philpot tapped as new Tas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>developingjen</td>\n",
       "      <td>Sean Parker, a Facebook and Napster Pioneer, t...</td>\n",
       "      <td>1</td>\n",
       "      <td>Sean Parker, a Facebook and Napster Pioneer, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>developingjen</td>\n",
       "      <td>@harper Christian Loffler, Veiled Grey. Massiv...</td>\n",
       "      <td>1</td>\n",
       "      <td>Christian Loffler, Veiled Grey. Massive Attac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>developingjen</td>\n",
       "      <td>@harper Peter Broderick: With Notes In My Ears...</td>\n",
       "      <td>1</td>\n",
       "      <td>Peter Broderick: With Notes In My Ears, Below...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              user                                               text  gender  \\\n",
       "175  developingjen  Best one yet // Why the Conversation About Fur...       1   \n",
       "176  developingjen  Recent reflections that are really impacting m...       1   \n",
       "177  developingjen               I love this! https://t.co/rRmaDxjqRg       1   \n",
       "178  developingjen  Really great piece by @dgh.  So much wisdom. h...       1   \n",
       "179  developingjen  The Further Future festival last weekend got m...       1   \n",
       "180  developingjen               @iRowan thank you again for this :).       1   \n",
       "181  developingjen            cc @rosenstein  https://t.co/JJk8xKeihx       1   \n",
       "182  developingjen  Thank you @iRowan for making this your cover s...       1   \n",
       "183  developingjen  Fascinating // This Mark Cuban-Backed Startup ...       1   \n",
       "184  developingjen  BBC News - Italian court rules food theft 'not...       1   \n",
       "185  developingjen  Best track ever? https://t.co/cqSHuILNVJ cc @h...       1   \n",
       "186  developingjen  What If We Just Gave Poor People a Basic Incom...       1   \n",
       "187  developingjen           This is exciting https://t.co/hMCUA74bdI       1   \n",
       "188  developingjen  #NowPlaying Relax &amp; Focus by Spotify // Pe...       1   \n",
       "189  developingjen                  #vacation https://t.co/xQCKam3LBX       1   \n",
       "190  developingjen  Fascinating. // This is Your Brain on LSD http...       1   \n",
       "191  developingjen  THIS! // Stacy Brown-Philpot tapped as new Tas...       1   \n",
       "192  developingjen  Sean Parker, a Facebook and Napster Pioneer, t...       1   \n",
       "193  developingjen  @harper Christian Loffler, Veiled Grey. Massiv...       1   \n",
       "194  developingjen  @harper Peter Broderick: With Notes In My Ears...       1   \n",
       "\n",
       "                                            clean_text  \n",
       "175  Best one yet // Why the Conversation About Fur...  \n",
       "176  Recent reflections that are really impacting m...  \n",
       "177               I love this! https://t.co/rRmaDxjqRg  \n",
       "178  Really great piece by .  So much wisdom. https...  \n",
       "179  The Further Future festival last weekend got m...  \n",
       "180                       thank you again for this :).  \n",
       "181                       cc   https://t.co/JJk8xKeihx  \n",
       "182  Thank you  for making this your cover story fo...  \n",
       "183  Fascinating // This Mark Cuban-Backed Startup ...  \n",
       "184  BBC News - Italian court rules food theft 'not...  \n",
       "185       Best track ever? https://t.co/cqSHuILNVJ cc   \n",
       "186  What If We Just Gave Poor People a Basic Incom...  \n",
       "187           This is exciting https://t.co/hMCUA74bdI  \n",
       "188   Relax  Focus by Spotify // Perfect playlist w...  \n",
       "189                            https://t.co/xQCKam3LBX  \n",
       "190  Fascinating. // This is Your Brain on LSD http...  \n",
       "191  THIS! // Stacy Brown-Philpot tapped as new Tas...  \n",
       "192  Sean Parker, a Facebook and Napster Pioneer, t...  \n",
       "193   Christian Loffler, Veiled Grey. Massive Attac...  \n",
       "194   Peter Broderick: With Notes In My Ears, Below...  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean text by removing things\n",
    "def remove_pattern(text,pattern):\n",
    "    \n",
    "    # re.findall() finds the pattern i.e @user and puts it in a list for further task\n",
    "    r = re.findall(pattern,text)\n",
    "    \n",
    "    # re.sub() removes @user from the sentences in the dataset\n",
    "    for i in r:\n",
    "        text = re.sub(i,\"\",text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "#Remove @, &, # and everything that follows\n",
    "tweets['clean_text'] = np.vectorize(remove_pattern)(tweets['text'], \"@[\\w]*\") # Removes all @\n",
    "tweets['clean_text'] = np.vectorize(remove_pattern)(tweets['clean_text'], \"&amp;\") # Removes all &\n",
    "tweets['clean_text'] = np.vectorize(remove_pattern)(tweets['clean_text'], \"#[\\w]*\") # Removes all #\n",
    "tweets.sample(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>gender</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>girlygeekdom</td>\n",
       "      <td>@WorldOfOrdinary @rmcopywriting @treekahlo @Je...</td>\n",
       "      <td>1</td>\n",
       "      <td>I'm seriously considering the self represe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>SHAQ</td>\n",
       "      <td>.@djdiesel is now on Twitter https://t.co/hYvw...</td>\n",
       "      <td>0</td>\n",
       "      <td>. is now on Twitter https://t.co/hYvwhfRx3y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>MariahCarey</td>\n",
       "      <td>@Deborah_Cox Thank you Deborah!! Love you and ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Thank you Deborah!! Love you and your beautif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>aliciakeys</td>\n",
       "      <td>New York strong https://t.co/h2NK8oL8kl</td>\n",
       "      <td>1</td>\n",
       "      <td>New York strong https://t.co/h2NK8oL8kl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>ladygaga</td>\n",
       "      <td>https://t.co/G9sEK0Uo6r</td>\n",
       "      <td>1</td>\n",
       "      <td>https://t.co/G9sEK0Uo6r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>jimmyfallon</td>\n",
       "      <td>It's time for Tonight Show: At Home Edition Ha...</td>\n",
       "      <td>0</td>\n",
       "      <td>It's time for Tonight Show: At Home Edition Ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Oprah</td>\n",
       "      <td>Sign up at https://t.co/B35nsuXHrX and I‚Äôll se...</td>\n",
       "      <td>1</td>\n",
       "      <td>Sign up at https://t.co/B35nsuXHrX and I‚Äôll se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>LilTunechi</td>\n",
       "      <td>IT‚ÄôS THAT GKUA Ultra Premium @gkuaofficial #th...</td>\n",
       "      <td>0</td>\n",
       "      <td>IT‚ÄôS THAT GKUA Ultra Premium    https://t.co/w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>AdamMGrant</td>\n",
       "      <td>For every Tiger Woods who specializes early, t...</td>\n",
       "      <td>0</td>\n",
       "      <td>For every Tiger Woods who specializes early, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>ruthreichl</td>\n",
       "      <td>Why restaurants matter - even to those who don...</td>\n",
       "      <td>1</td>\n",
       "      <td>Why restaurants matter - even to those who don...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>akshaykumar</td>\n",
       "      <td>üôèüèªüôèüèª https://t.co/IqRdxrQ2W9</td>\n",
       "      <td>0</td>\n",
       "      <td>üôèüèªüôèüèª https://t.co/IqRdxrQ2W9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>davidguetta</td>\n",
       "      <td>Me too :))) https://t.co/DDZ5j4rKjF</td>\n",
       "      <td>0</td>\n",
       "      <td>Me too :))) https://t.co/DDZ5j4rKjF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>girlygeekdom</td>\n",
       "      <td>@zsk The older one has been writing letters to...</td>\n",
       "      <td>1</td>\n",
       "      <td>The older one has been writing letters to fri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>narendramodi</td>\n",
       "      <td>India stands with Russia in solemn remembrance...</td>\n",
       "      <td>0</td>\n",
       "      <td>India stands with Russia in solemn remembrance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>BeingSalmanKhan</td>\n",
       "      <td>Congrats on becoming aspiring shot yet again! ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Congrats on becoming aspiring shot yet again! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>BarackObama</td>\n",
       "      <td>We‚Äôve seen the power that our voices have when...</td>\n",
       "      <td>0</td>\n",
       "      <td>We‚Äôve seen the power that our voices have when...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>kenyanpundit</td>\n",
       "      <td>The Empire strikes back. üòÇ https://t.co/TNPKl2...</td>\n",
       "      <td>1</td>\n",
       "      <td>The Empire strikes back. üòÇ https://t.co/TNPKl2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>aaker</td>\n",
       "      <td>Mental contrasting: What happens when you comb...</td>\n",
       "      <td>1</td>\n",
       "      <td>Mental contrasting: What happens when you comb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>ArianaGrande</td>\n",
       "      <td>@AlfredoFlores ü§ç</td>\n",
       "      <td>1</td>\n",
       "      <td>ü§ç</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>akshaykumar</td>\n",
       "      <td>Request to vote for Mumbai, \\nMumbaikars, visi...</td>\n",
       "      <td>0</td>\n",
       "      <td>Request to vote for Mumbai, \\nMumbaikars, visi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                user                                               text  \\\n",
       "147     girlygeekdom  @WorldOfOrdinary @rmcopywriting @treekahlo @Je...   \n",
       "110             SHAQ  .@djdiesel is now on Twitter https://t.co/hYvw...   \n",
       "136      MariahCarey  @Deborah_Cox Thank you Deborah!! Love you and ...   \n",
       "85        aliciakeys            New York strong https://t.co/h2NK8oL8kl   \n",
       "88          ladygaga                            https://t.co/G9sEK0Uo6r   \n",
       "150      jimmyfallon  It's time for Tonight Show: At Home Edition Ha...   \n",
       "11             Oprah  Sign up at https://t.co/B35nsuXHrX and I‚Äôll se...   \n",
       "111       LilTunechi  IT‚ÄôS THAT GKUA Ultra Premium @gkuaofficial #th...   \n",
       "57        AdamMGrant  For every Tiger Woods who specializes early, t...   \n",
       "52        ruthreichl  Why restaurants matter - even to those who don...   \n",
       "77       akshaykumar                       üôèüèªüôèüèª https://t.co/IqRdxrQ2W9   \n",
       "8        davidguetta                Me too :))) https://t.co/DDZ5j4rKjF   \n",
       "6       girlygeekdom  @zsk The older one has been writing letters to...   \n",
       "144     narendramodi  India stands with Russia in solemn remembrance...   \n",
       "152  BeingSalmanKhan  Congrats on becoming aspiring shot yet again! ...   \n",
       "0        BarackObama  We‚Äôve seen the power that our voices have when...   \n",
       "66      kenyanpundit  The Empire strikes back. üòÇ https://t.co/TNPKl2...   \n",
       "129            aaker  Mental contrasting: What happens when you comb...   \n",
       "95      ArianaGrande                                   @AlfredoFlores ü§ç   \n",
       "89       akshaykumar  Request to vote for Mumbai, \\nMumbaikars, visi...   \n",
       "\n",
       "     gender                                         clean_text  \n",
       "147       1      I'm seriously considering the self represe...  \n",
       "110       0        . is now on Twitter https://t.co/hYvwhfRx3y  \n",
       "136       1   Thank you Deborah!! Love you and your beautif...  \n",
       "85        1            New York strong https://t.co/h2NK8oL8kl  \n",
       "88        1                            https://t.co/G9sEK0Uo6r  \n",
       "150       0  It's time for Tonight Show: At Home Edition Ha...  \n",
       "11        1  Sign up at https://t.co/B35nsuXHrX and I‚Äôll se...  \n",
       "111       0  IT‚ÄôS THAT GKUA Ultra Premium    https://t.co/w...  \n",
       "57        0  For every Tiger Woods who specializes early, t...  \n",
       "52        1  Why restaurants matter - even to those who don...  \n",
       "77        0                       üôèüèªüôèüèª https://t.co/IqRdxrQ2W9  \n",
       "8         0                Me too :))) https://t.co/DDZ5j4rKjF  \n",
       "6         1   The older one has been writing letters to fri...  \n",
       "144       0  India stands with Russia in solemn remembrance...  \n",
       "152       0  Congrats on becoming aspiring shot yet again! ...  \n",
       "0         0  We‚Äôve seen the power that our voices have when...  \n",
       "66        1  The Empire strikes back. üòÇ https://t.co/TNPKl2...  \n",
       "129       1  Mental contrasting: What happens when you comb...  \n",
       "95        1                                                  ü§ç  \n",
       "89        0  Request to vote for Mumbai, \\nMumbaikars, visi...  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.sample(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>gender</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>susanmcp1</td>\n",
       "      <td>Update, we raised $12.5k from donations means ...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>NiallOfficial</td>\n",
       "      <td>thanks @applemusic for putting me on the cover...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>SrBachchan</td>\n",
       "      <td>@SwetaLoveAB @artistrishika üåπ</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>selenagomez</td>\n",
       "      <td>And the final song on Rare‚Ä¶ A Sweeter Place ft...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>NiallOfficial</td>\n",
       "      <td>@cariadoresyou Hahah no worries</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>JLo</td>\n",
       "      <td>¬°Gracias @PeopleEnEspanol! ‚ú®üñ§ü§ç ‚ú® Los 50 m√°s be...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>LilTunechi</td>\n",
       "      <td>We lost a King.  824</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>narendramodi</td>\n",
       "      <td>Today was the 4th interaction with CMs. We con...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>icecube</td>\n",
       "      <td>It ain‚Äôt gonna turn out how they think... http...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>drdre</td>\n",
       "      <td>Can‚Äôt wait to have @QuincyDJones on #ThePharma...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>chefannc</td>\n",
       "      <td>#realschoolfood - the Chefs are serving the at...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>aliciakeys</td>\n",
       "      <td>Wooooowww!!!! You for are deep inspirations of...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>LilTunechi</td>\n",
       "      <td>We‚Äôre goin live from my studio at 4pm PT/7pm E...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>SrBachchan</td>\n",
       "      <td>T 3541 - Wear the mask .. an initiative by @av...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>Drake</td>\n",
       "      <td>Rest in peace Yams. A$AP is family.</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>carmeloanthony</td>\n",
       "      <td>@MaisonValentino \\n#valentinoresort18 ‚ÄúJust Th...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>britneyspears</td>\n",
       "      <td>Couldn't make up mind... hair up or down??? üë†üë†...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>susanmcp1</td>\n",
       "      <td>@AllanaHarkin beyond gross</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>leila_c</td>\n",
       "      <td>@DevSpoke What‚Äôs crazy is how no one talks abo...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>BarackObama</td>\n",
       "      <td>Here‚Äôs to a happy, healthy, and hopeful 2020. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               user                                               text  \\\n",
       "16        susanmcp1  Update, we raised $12.5k from donations means ...   \n",
       "131   NiallOfficial  thanks @applemusic for putting me on the cover...   \n",
       "72       SrBachchan                      @SwetaLoveAB @artistrishika üåπ   \n",
       "143     selenagomez  And the final song on Rare‚Ä¶ A Sweeter Place ft...   \n",
       "39    NiallOfficial                    @cariadoresyou Hahah no worries   \n",
       "108             JLo  ¬°Gracias @PeopleEnEspanol! ‚ú®üñ§ü§ç ‚ú® Los 50 m√°s be...   \n",
       "92       LilTunechi                               We lost a King.  824   \n",
       "195    narendramodi  Today was the 4th interaction with CMs. We con...   \n",
       "25          icecube  It ain‚Äôt gonna turn out how they think... http...   \n",
       "23            drdre  Can‚Äôt wait to have @QuincyDJones on #ThePharma...   \n",
       "120        chefannc  #realschoolfood - the Chefs are serving the at...   \n",
       "52       aliciakeys  Wooooowww!!!! You for are deep inspirations of...   \n",
       "54       LilTunechi  We‚Äôre goin live from my studio at 4pm PT/7pm E...   \n",
       "68       SrBachchan  T 3541 - Wear the mask .. an initiative by @av...   \n",
       "114           Drake                Rest in peace Yams. A$AP is family.   \n",
       "173  carmeloanthony  @MaisonValentino \\n#valentinoresort18 ‚ÄúJust Th...   \n",
       "112   britneyspears  Couldn't make up mind... hair up or down??? üë†üë†...   \n",
       "24        susanmcp1                         @AllanaHarkin beyond gross   \n",
       "129         leila_c  @DevSpoke What‚Äôs crazy is how no one talks abo...   \n",
       "121     BarackObama  Here‚Äôs to a happy, healthy, and hopeful 2020. ...   \n",
       "\n",
       "     gender clean_text  \n",
       "16        1        NaN  \n",
       "131       0        NaN  \n",
       "72        0        NaN  \n",
       "143       1        NaN  \n",
       "39        0        NaN  \n",
       "108       1        NaN  \n",
       "92        0        NaN  \n",
       "195       0        NaN  \n",
       "25        0        NaN  \n",
       "23        0        NaN  \n",
       "120       1        NaN  \n",
       "52        1        NaN  \n",
       "54        0        NaN  \n",
       "68        0        NaN  \n",
       "114       0        NaN  \n",
       "173       0        NaN  \n",
       "112       1        NaN  \n",
       "24        1        NaN  \n",
       "129       1        NaN  \n",
       "121       0        NaN  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['clean_text'] = tweets['clean_text'].str.extract('(.*)http?')\n",
    "tweets.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>gender</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>BarackObama</td>\n",
       "      <td>We‚Äôve seen the power that our voices have when...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>BarackObama</td>\n",
       "      <td>On National Gun Violence Awareness Day, we #We...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>BarackObama</td>\n",
       "      <td>Third, every city in this country should be a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>BarackObama</td>\n",
       "      <td>Second, every mayor should review their use of...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>BarackObama</td>\n",
       "      <td>First, there are specific evidence-based refor...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>developingjen</td>\n",
       "      <td>Fascinating. // This is Your Brain on LSD http...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>developingjen</td>\n",
       "      <td>THIS! // Stacy Brown-Philpot tapped as new Tas...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>developingjen</td>\n",
       "      <td>Sean Parker, a Facebook and Napster Pioneer, t...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>developingjen</td>\n",
       "      <td>@harper Christian Loffler, Veiled Grey. Massiv...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>developingjen</td>\n",
       "      <td>@harper Peter Broderick: With Notes In My Ears...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12919 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              user                                               text  gender  \\\n",
       "0      BarackObama  We‚Äôve seen the power that our voices have when...       0   \n",
       "1      BarackObama  On National Gun Violence Awareness Day, we #We...       0   \n",
       "2      BarackObama  Third, every city in this country should be a ...       0   \n",
       "3      BarackObama  Second, every mayor should review their use of...       0   \n",
       "4      BarackObama  First, there are specific evidence-based refor...       0   \n",
       "..             ...                                                ...     ...   \n",
       "190  developingjen  Fascinating. // This is Your Brain on LSD http...       1   \n",
       "191  developingjen  THIS! // Stacy Brown-Philpot tapped as new Tas...       1   \n",
       "192  developingjen  Sean Parker, a Facebook and Napster Pioneer, t...       1   \n",
       "193  developingjen  @harper Christian Loffler, Veiled Grey. Massiv...       1   \n",
       "194  developingjen  @harper Peter Broderick: With Notes In My Ears...       1   \n",
       "\n",
       "    clean_text  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  \n",
       "..         ...  \n",
       "190        NaN  \n",
       "191        NaN  \n",
       "192        NaN  \n",
       "193        NaN  \n",
       "194        NaN  \n",
       "\n",
       "[12919 rows x 4 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>gender</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>BarackObama</td>\n",
       "      <td>We‚Äôve seen the power that our voices have when...</td>\n",
       "      <td>0</td>\n",
       "      <td>We‚Äôve seen the power that our voices have when...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>BarackObama</td>\n",
       "      <td>On National Gun Violence Awareness Day, we #We...</td>\n",
       "      <td>0</td>\n",
       "      <td>On National Gun Violence Awareness Day, we  to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>BarackObama</td>\n",
       "      <td>Third, every city in this country should be a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Third, every city in this country should be a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>BarackObama</td>\n",
       "      <td>Second, every mayor should review their use of...</td>\n",
       "      <td>0</td>\n",
       "      <td>Second, every mayor should review their use of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>BarackObama</td>\n",
       "      <td>First, there are specific evidence-based refor...</td>\n",
       "      <td>0</td>\n",
       "      <td>First, there are specific evidence-based refor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>developingjen</td>\n",
       "      <td>Fascinating. // This is Your Brain on LSD http...</td>\n",
       "      <td>1</td>\n",
       "      <td>Fascinating. // This is Your Brain on LSD http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>developingjen</td>\n",
       "      <td>THIS! // Stacy Brown-Philpot tapped as new Tas...</td>\n",
       "      <td>1</td>\n",
       "      <td>THIS! // Stacy Brown-Philpot tapped as new Tas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>developingjen</td>\n",
       "      <td>Sean Parker, a Facebook and Napster Pioneer, t...</td>\n",
       "      <td>1</td>\n",
       "      <td>Sean Parker, a Facebook and Napster Pioneer, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>developingjen</td>\n",
       "      <td>@harper Christian Loffler, Veiled Grey. Massiv...</td>\n",
       "      <td>1</td>\n",
       "      <td>Christian Loffler, Veiled Grey. Massive Attac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>developingjen</td>\n",
       "      <td>@harper Peter Broderick: With Notes In My Ears...</td>\n",
       "      <td>1</td>\n",
       "      <td>Peter Broderick: With Notes In My Ears, Below...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12919 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              user                                               text  gender  \\\n",
       "0      BarackObama  We‚Äôve seen the power that our voices have when...       0   \n",
       "1      BarackObama  On National Gun Violence Awareness Day, we #We...       0   \n",
       "2      BarackObama  Third, every city in this country should be a ...       0   \n",
       "3      BarackObama  Second, every mayor should review their use of...       0   \n",
       "4      BarackObama  First, there are specific evidence-based refor...       0   \n",
       "..             ...                                                ...     ...   \n",
       "190  developingjen  Fascinating. // This is Your Brain on LSD http...       1   \n",
       "191  developingjen  THIS! // Stacy Brown-Philpot tapped as new Tas...       1   \n",
       "192  developingjen  Sean Parker, a Facebook and Napster Pioneer, t...       1   \n",
       "193  developingjen  @harper Christian Loffler, Veiled Grey. Massiv...       1   \n",
       "194  developingjen  @harper Peter Broderick: With Notes In My Ears...       1   \n",
       "\n",
       "                                            clean_text  \n",
       "0    We‚Äôve seen the power that our voices have when...  \n",
       "1    On National Gun Violence Awareness Day, we  to...  \n",
       "2    Third, every city in this country should be a ...  \n",
       "3    Second, every mayor should review their use of...  \n",
       "4    First, there are specific evidence-based refor...  \n",
       "..                                                 ...  \n",
       "190  Fascinating. // This is Your Brain on LSD http...  \n",
       "191  THIS! // Stacy Brown-Philpot tapped as new Tas...  \n",
       "192  Sean Parker, a Facebook and Napster Pioneer, t...  \n",
       "193   Christian Loffler, Veiled Grey. Massive Attac...  \n",
       "194   Peter Broderick: With Notes In My Ears, Below...  \n",
       "\n",
       "[12919 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean text by removing things\n",
    "def remove_pattern(text,pattern):\n",
    "    \n",
    "    # re.findall() finds the pattern i.e @user and puts it in a list for further task\n",
    "    r = re.findall(pattern,text)\n",
    "    \n",
    "    # re.sub() removes @user from the sentences in the dataset\n",
    "    for i in r:\n",
    "        text = re.sub(i,\"\",text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "#Remove @ symbol, URL links, and \"&amp;\"\n",
    "tweets['clean_text'] = np.vectorize(remove_pattern)(tweets['text'], \"@[\\w]*\") #removes all @\n",
    "tweets['clean_text'] = np.vectorize(remove_pattern)(tweets['clean_text'], \"&amp;\")\n",
    "tweets['clean_text'] = np.vectorize(remove_pattern)(tweets['clean_text'], \"#[\\w]*\")\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "unbalanced parenthesis at position 23",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f18a5aa47644>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_pattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"&amp;\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_pattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"#[\\w]*\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#removes all hashtags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_pattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"https?:\\/\\/.*[\\r\\n]*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2089\u001b[0m             \u001b[0mvargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_n\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_n\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2091\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vectorize_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2093\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_ufunc_and_otypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_vectorize_call\u001b[0;34m(self, func, args)\u001b[0m\n\u001b[1;32m   2165\u001b[0m                       for a in args]\n\u001b[1;32m   2166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2167\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2169\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-3beb7ad92cf3>\u001b[0m in \u001b[0;36mremove_pattern\u001b[0;34m(text, pattern)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# re.sub() removes @user from the sentences in the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/re.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/re.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"first argument must be string or compiled pattern\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0m_MAXCACHE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/sre_compile.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(p, flags)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msre_parse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/sre_parse.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(str, flags, pattern)\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\")\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unbalanced parenthesis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mflags\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mSRE_FLAG_DEBUG\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: unbalanced parenthesis at position 23"
     ]
    }
   ],
   "source": [
    "tweets['clean_text'] = np.vectorize(remove_pattern)(tweets['clean_text'], \"&amp;\")\n",
    "tweets['clean_text'] = np.vectorize(remove_pattern)(tweets['clean_text'], \"#[\\w]*\") #removes all hashtags\n",
    "tweets['clean_text'] = np.vectorize(remove_pattern)(tweets['clean_text'], \"https?:\\/\\/.*[\\r\\n]*\")\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # reduce multiple spaces and newlines to only one\n",
    "    text = re.sub(r'(\\s\\s+|\\n\\n+)', r'\\1', text)\n",
    "    # remove double quotes\n",
    "    text = re.sub(r'\"', '', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['clean_text'] = tweets['text'].apply(clean_text)\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "\n",
    "def convert_text(text):\n",
    "    sent = nlp(text)\n",
    "    ents = {x.text: x for x in sent.ents}\n",
    "    tokens = []\n",
    "    for w in sent:\n",
    "        if w.is_stop or w.is_punct:\n",
    "            continue\n",
    "        if w.text in ents:\n",
    "            tokens.append(w.text)\n",
    "        else:\n",
    "            tokens.append(w.lemma_.lower())\n",
    "    text = ' '.join(tokens)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['clean_text'] = tweets['clean_text'].apply(convert_text)\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['clean_text'] = tweets['text']\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text by removing things\n",
    "def remove_pattern(text,pattern):\n",
    "    \n",
    "    # re.findall() finds the pattern i.e @user and puts it in a list for further task\n",
    "    r = re.findall(pattern,text)\n",
    "    \n",
    "    # re.sub() removes @user from the sentences in the dataset\n",
    "    for i in r:\n",
    "        text = re.sub(i,\"\",text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "#Remove @ symbol, URL links, and \"&amp;\"\n",
    "tweets['clean_text'] = np.vectorize(remove_pattern)(tweets['text'], \"@[\\w]*\") #removes all @\n",
    "tweets['clean_text'] = np.vectorize(remove_pattern)(tweets['clean_text'], \"&amp;\")\n",
    "tweets['clean_text'] = np.vectorize(remove_pattern)(tweets['clean_text'], \"#[\\w]*\") #removes all hashtags\n",
    "tweets['clean_text'] = np.vectorize(remove_pattern)(tweets['clean_text'], \"https:\\/\\/.*[\\r\\n]*\")\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = pd.read_csv('../Data/twitter-test.csv')\n",
    "testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for the testing dataset\n",
    "# Clean text by removing things\n",
    "testset['clean_text'] = np.vectorize(remove_pattern)(testset['text'], \"@[\\w]*\") #removes all @\n",
    "testset['clean_text'] = np.vectorize(remove_pattern)(testset['clean_text'], \"https?:\\/\\/.*[\\r\\n]*\")\n",
    "testset['clean_text'] = np.vectorize(remove_pattern)(testset['clean_text'], \"&amp;\")\n",
    "testset['clean_text'] = np.vectorize(remove_pattern)(testset['clean_text'], \"#[\\w]*\") #removes all hashtags\n",
    "testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation, numbers, and special characters\n",
    "tweets['clean_text'] = tweets['clean_text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for the testing dataset\n",
    "# Remove punctuation, numbers, and special characters\n",
    "testset['clean_text'] = testset['clean_text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove short words less than 3\n",
    "tweets['clean_text'] = tweets['clean_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "tweets.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for the testing dataset\n",
    "# Remove short words less than 2\n",
    "testset['clean_text'] = testset['clean_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "testset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new column to count length of clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count length of characters\n",
    "tweets['length'] = tweets['clean_text'].apply(len)\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for the testing dataset\n",
    "# Count length\n",
    "testset['length'] = testset['clean_text'].apply(len)\n",
    "testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rows in training data that have less than desired text length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(tweets['length'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where length <= 30\n",
    "tweets = tweets[tweets.length > 30]\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(tweets['length'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize, stem, and stich back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # reduce multiple spaces and newlines to only one\n",
    "    text = re.sub(r'(\\s\\s+|\\n\\n+)', r'\\1', text)\n",
    "    # remove double quotes\n",
    "    text = re.sub(r'\"', '', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['clean_text2'] = tweets['text'].apply(clean_text)\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text(text):\n",
    "    sent = nlp(text)\n",
    "    ents = {x.text: x for x in sent.ents}\n",
    "    tokens = []\n",
    "    for w in sent:\n",
    "        if w.is_stop or w.is_punct:\n",
    "            continue\n",
    "        if w.text in ents:\n",
    "            tokens.append(w.text)\n",
    "        else:\n",
    "            tokens.append(w.lemma_.lower())\n",
    "    text = ' '.join(tokens)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['clean_text2'] = tweets['text'].apply(convert_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google 'pandas' .apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to capitalize all characters\n",
    "def capitalize(x):\n",
    "    return x.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'allen'\n",
    "capitalize(test)\n",
    "# do this for df column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda is a one use function that you don't need to define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tokenization\n",
    "# tokenized_tweet = tweets['clean_text'].apply(lambda x: x.split())\n",
    "# tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Do the same for the testing dataset\n",
    "# # Tokenization\n",
    "# tokenized_testset = testset['clean_text'].apply(lambda x: x.split())\n",
    "# tokenized_testset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stemming\n",
    "# ps = PorterStemmer()\n",
    "# tokenized_tweet = tokenized_tweet.apply(lambda x: [ps.stem(i) for i in x])\n",
    "# tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Do the same for the testing dataset\n",
    "# # Stemming\n",
    "# ps = PorterStemmer()\n",
    "# tokenized_testset = tokenized_testset.apply(lambda x: [ps.stem(i) for i in x])\n",
    "# tokenized_testset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stich tokens back together\n",
    "# for i in range(len(tokenized_tweet)):\n",
    "#     tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "          \n",
    "# testset['clean_text'] = tokenized_tweet\n",
    "# testset['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stich tokens back together\n",
    "# for i in range(len(tokenized_tweet)):\n",
    "#     tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "          \n",
    "# testset['clean_text'] = tokenized_tweet\n",
    "# testset['clean_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Selectioin and Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-Words features\n",
    "bow_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Bag-of-Words feature matrix\n",
    "bow = bow_vectorizer.fit_transform(tweets['clean_text'])\n",
    "df_bow = pd.DataFrame(bow.todense(), columns=bow_vectorizer.get_feature_names())\n",
    "df_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for test dataset\n",
    "# Bag-of-Words feature matrix\n",
    "bow = bow_vectorizer.transform(testset['clean_text'])\n",
    "df_bow_test = pd.DataFrame(bow.todense(), columns=bow_vectorizer.get_feature_names())\n",
    "df_bow_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Bag of Words to Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and validation set\n",
    "X = df_bow\n",
    "y = tweets['gender']\n",
    "\n",
    "# Use Bag-of-Words Features\n",
    "X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting on Logistic Regression model\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_bow, y_train_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first part of the list is predicting probabilities for gender:0 (male)\n",
    "# The second part of the list is predicting probabilities for gender:1 (female)\n",
    "prediction_bow = logreg.predict_proba(X_test_bow)\n",
    "prediction_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the F1 score\n",
    "# If prediction is greater than or equal to 0.3 than 1, else 0\n",
    "# Where 0 is for male tweets and 1 is for female tweets\n",
    "prediction_int = prediction_bow[:,1]>=0.5\n",
    "\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "prediction_int\n",
    "\n",
    "# Calculating f1 score\n",
    "log_bow = f1_score(y_test_bow, prediction_int)\n",
    "\n",
    "log_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict with separate test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there is a fit model\n",
    "logreg.intercept_, logreg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = df_bow_test\n",
    "pred = logreg.predict_proba(z)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2 = logreg.predict(z)\n",
    "pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=pred)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2 = pd.DataFrame(data=pred2, columns=['predicted_gender'])\n",
    "pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.join(pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF features (Term Frequency-Inverse Document Frequency)\n",
    "tfidf=TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix=tfidf.fit_transform(tweets['clean_text'])\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.todense(), columns=tfidf.get_feature_names())\n",
    "df_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for the test dataset\n",
    "# TF-IDF features (Term Frequency-Inverse Document Frequency)\n",
    "tfidf_matrix=tfidf.transform(testset['clean_text'])\n",
    "df_tfidf_test = pd.DataFrame(tfidf_matrix.todense(), columns=tfidf.get_feature_names())\n",
    "df_tfidf_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use TF-IDF to Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and validation set\n",
    "X = df_tfidf\n",
    "y = tweets['gender']\n",
    "\n",
    "# Use Bag-of-Words Features\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using TF-IDF Features\n",
    "logreg.fit(X_train_tfidf, y_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_tfidf = logreg.predict_proba(X_test_tfidf)\n",
    "prediction_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the F1 score\n",
    "prediction_int = prediction_tfidf[:,1]>=0.5\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "prediction_int\n",
    "\n",
    "# calculating f1 score\n",
    "log_tfidf = f1_score(y_test_tfidf, prediction_int)\n",
    "log_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "dtc = DecisionTreeClassifier(criterion='entropy', random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Bag of Words as features\n",
    "dtc.fit(X_train_bow, y_train_bow)\n",
    "dtc_bow = dtc.predict_proba(X_test_bow)\n",
    "dtc_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if prediction is greater than or equal to 0.3 than 1 else 0\n",
    "# Where 0 is for positive sentiment tweets and 1 for negative sentiment tweets\n",
    "dtc_bow = dtc_bow[:,1]>=0.5\n",
    "\n",
    "# converting the results to integer type\n",
    "dtc_int_bow=dtc_bow.astype(np.int)\n",
    "\n",
    "# calculating f1 score\n",
    "dtc_score_bow=f1_score(y_test_bow, dtc_int_bow)\n",
    "\n",
    "dtc_score_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using TF-IDF\n",
    "dtc.fit(x_train_tfidf,y_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_tfidf = dtc.predict_proba(X_test_tfidf)\n",
    "\n",
    "dtc_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if prediction is greater than or equal to 0.3 than 1 else 0\n",
    "# Where 0 is for positive sentiment tweets and 1 for negative sentiment tweets\n",
    "dtc_tfidf=dtc_tfidf[:,1]>=0.3\n",
    "\n",
    "# converting the results to integer type\n",
    "dtc_int_tfidf=dtc_tfidf.astype(np.int)\n",
    "\n",
    "# calculating f1 score\n",
    "dtc_score_tfidf=f1_score(y_test_tfidf,dtc_int_tfidf)\n",
    "\n",
    "dtc_score_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison\n",
    "Algo=['LogisticRegression(Bag-of-Words)','DecisionTree(Bag-of-Words)','LogisticRegression(TF-IDF)','DecisionTree(TF-IDF)']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = [log_bow,dct_score_bow,log_tfidf,dct_score_tfidf]\n",
    "\n",
    "compare=pd.DataFrame({'Model':Algo,'F1_Score':score},index=[i for i in range(1,5)])\n",
    "compare.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,5))\n",
    "\n",
    "sns.pointplot(x='Model',y='F1_Score',data=compare)\n",
    "\n",
    "plt.title('Model Vs Score')\n",
    "plt.xlabel('MODEL')\n",
    "plt.ylabel('SCORE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test With Real Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there is a fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Log_Reg.intercept_, Log_Reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = pd.read_csv('../Data/tweetstest.csv')\n",
    "test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = bow_vectorizer.transform(test_text['clean_text']) #use .transform() not .fit_transform()\n",
    "df_bow = pd.DataFrame(bow.todense())\n",
    "df_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_bow = Log_Reg.predict_proba(X)\n",
    "prediction_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"this is a test tweet to predict my gender baby boo\"\n",
    "\n",
    "# Bag-of-Words feature matrix\n",
    "bow = bow_vectorizer.fit_transform('test_text')\n",
    "df_bow = pd.DataFrame(bow.todense())\n",
    "df_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I am so angry\"\n",
    "textBlob = TextBlob(text)\n",
    "print(f\"{textBlob.sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
